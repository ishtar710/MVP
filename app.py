import streamlit as st
import json
import requests
import openai
import base64
from typing import List, Dict, Any
from openai import AzureOpenAI

# Azure OpenAI ì„¤ì •
AZURE_OPENAI_ENDPOINT = "https://your-azure-openai-endpoint.openai.azure.com/"
AZURE_DEPLOYMENT_MODEL = "dev-gpt-4.1-mini"
AZURE_OPENAI_API_KEY = ""


try:
    # Initialize the OpenAI client
    openai_client = AzureOpenAI(
        api_version="2024-12-01-preview",
        api_key=AZURE_OPENAI_API_KEY,
        azure_endpoint=AZURE_OPENAI_ENDPOINT
    )
except Exception as e:
    st.error(f"OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
    st.stop()

# í•¨ìˆ˜: Parse í”„ë¡¬í”„íŠ¸ ìƒì„±
def build_parse_prompt(trace_logs: List[Dict[str, Any]]) -> List[Dict[str, str]]:
    """
    LLM Parse ë‹¨ê³„ì— ë„£ì„ í”„ë¡¬í”„íŠ¸ ë©”ì‹œì§€ ìƒì„±
    - ë°˜ë“œì‹œ JSONë§Œ ë°˜í™˜í•˜ë„ë¡ ì§€ì‹œ
    """
    return [
        {
            "role": "system",
            "content": (
                "ë‹¹ì‹ ì€ PICASO ë¡œê·¸ë¥¼ ì‹œí€€ìŠ¤ ë‹¤ì´ì–´ê·¸ë¨ stepìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ë¶„ì„ê¸°ì…ë‹ˆë‹¤.\n"
                "ì¶œë ¥ì€ ë°˜ë“œì‹œ JSONë§Œ ë°˜í™˜í•´ì•¼ í•˜ë©°, ë‹¤ë¥¸ ì„¤ëª…ì´ë‚˜ í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.\n"
                "JSON ì´ì™¸ì˜ í…ìŠ¤íŠ¸(ì˜ˆ: ì„¤ëª…, ì£¼ì„)ëŠ” ì ˆëŒ€ ì¶œë ¥í•˜ì§€ ë§ˆì‹­ì‹œì˜¤. ``` ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ì½”ë“œ ë¸”ë¡ë„ í¬í•¨í•˜ì§€ ë§ˆì‹­ì‹œì˜¤."
            )
        },
        {
            "role": "user",
            "content": f"""
ì•„ë˜ëŠ” ë™ì¼ transactionIdì— ì†í•˜ëŠ” ë¡œê·¸ í•­ëª©ë“¤ì…ë‹ˆë‹¤.
ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ IN-REQ â†’ OUT-REQ â†’ OUT-RES â†’ IN-RES stepì„ ì¶”ë¡ í•˜ì—¬ JSONìœ¼ë¡œ ë°˜í™˜í•˜ì‹œì˜¤.

ê·œì¹™:
1. ê° íŠ¸ëœì­ì…˜ì€ ë°˜ë“œì‹œ IN-REQ â†’ OUT-REQ â†’ OUT-RES â†’ IN-RES ìˆœì„œë¡œ êµ¬ì„±ëœë‹¤.
   - IN-REQ/IN-RESëŠ” ê°ê° í•˜ë‚˜ë§Œ ì¡´ì¬í•œë‹¤.
   - OUT-REQ/OUT-RESëŠ” ì—¬ëŸ¬ ê°œ ì¡´ì¬í•  ìˆ˜ ìˆë‹¤.
   - ì¤‘ê°„ stepì´ ëˆ„ë½ë  ìˆ˜ ìˆìœ¼ë©°, ì´ ê²½ìš° "MISSING" stepìœ¼ë¡œ ì±„ìš´ë‹¤.

2. ë¡œê·¸ì—ëŠ” REQê°€ ê¸°ë¡ë˜ì§€ ì•Šê³  RES ë¡œê·¸ë§Œ ë‚¨ëŠ”ë‹¤.
   - logType=IN_RES â†’ IN-REQ + IN-RESë¥¼ ì¶”ë¡ í•´ì•¼ í•œë‹¤.
   - logType=OUT_RES â†’ OUT-REQ + OUT-RESë¥¼ ì¶”ë¡ í•´ì•¼ í•œë‹¤.

3. ë§¤í•‘ ê·œì¹™:
   - IN-REQì˜ actorëŠ” caller(ì™¸ë¶€ ì‹œìŠ¤í…œ), targetì€ PICASO.
   - OUT-REQì˜ actorëŠ” PICASO, targetì€ OUT-RESì˜ destination.
   - OUT-RESì˜ actorëŠ” OUT-REQì˜ target(ì¦‰, ì™¸ë¶€ ì‹œìŠ¤í…œ), targetì€ PICASO.
   - IN-RESì˜ actorëŠ” PICASO, targetì€ IN-REQì˜ actor(ì¦‰, caller).
   - caller ì •ë³´ëŠ” ìµœì´ˆ ìš”ì²­(IN-REQ)ë¶€í„° IN-RESê¹Œì§€ ë™ì¼í•˜ê²Œ ì—°ê²°ëœë‹¤.
   - IN-RESì˜ destinationê³¼ OUT-REQì˜ í˜¸ì¶œìëŠ” í•­ìƒ PICASOë¡œ ê³ ì •í•œë‹¤.

4. statusì™€ latency_ms:
   - statusëŠ” response.codeë¥¼ ì‚¬ìš©í•œë‹¤. ì—†ìœ¼ë©´ response.typeìœ¼ë¡œ ìœ ì¶”(Eâ†’500, Iâ†’200).
   - latency_msëŠ” response.durationì„ ì •ìˆ˜ë¡œ ë³€í™˜í•œë‹¤. ì—†ìœ¼ë©´ -1.

ì¶œë ¥ ìŠ¤í‚¤ë§ˆ:
{{
  "trace_id": "íŠ¸ëœì­ì…˜ID",
  "steps": [
    {{
      "step_no": 1,
      "type": "IN-REQ",
      "actor": "...",
      "action": "...",
      "target": "...",
      "status": "...",
      "latency_ms": 123
    }},
    ...
  ]
}}

ì…ë ¥ ë¡œê·¸:
{json.dumps(trace_logs, ensure_ascii=False, indent=2)}
"""
        }
    ]


# í•¨ìˆ˜: PlantUML ìƒì„± í”„ë¡¬í”„íŠ¸
# def build_generate_prompt(refined_steps):
#     return [
#         {"role": "system", "content": "ë‹¹ì‹ ì€ ì‹œí€€ìŠ¤ ë‹¤ì´ì–´ê·¸ë¨(PlantUML) ìƒì„±ê¸°ì…ë‹ˆë‹¤. ì…ë ¥ìœ¼ë¡œ ë°›ì€ refined_stepsë¥¼ PlantUML ë¬¸ë²•ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜í•˜ì‹œì˜¤."},
#         {"role": "user", "content": f"""
# ì‘ì—…:
# ì•„ë˜ refined_stepsë¥¼ PlantUML í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì‹œì˜¤. ê° í™”ì‚´í‘œ ë¼ë²¨ì— latency_ms í¬í•¨, statusê°€ 4xx/5xxì´ë©´ #red ì£¼ì„ ì¶”ê°€.
# IN-REQ, OUT-REQëŠ” í™”ì‚´í‘œ -> , IN-RES, OUT-RESëŠ” í™”ì‚´í‘œ --> ì‚¬ìš©.

# ì…ë ¥:
# {json.dumps(refined_steps, ensure_ascii=False)}

# ì¶œë ¥ ì˜ˆì‹œ:
# @startuml
# actor "Skylife-API"
# participant "PICASO-GW"
# Skylife-API -> PICASO-GW : POST /subscribe\n120ms
# @enduml
# """}
#     ]

# í•¨ìˆ˜: PlantUML ìƒì„± í”„ë¡¬í”„íŠ¸ (ìˆ˜ì •ë³¸: PlantUML ì½”ë“œë§Œ ë°˜í™˜ ê°•ì œ)
def build_generate_prompt(refined_steps: Dict[str, Any]) -> List[Dict[str, str]]:
    """
    LLM Generate ë‹¨ê³„ì— ë„£ì„ í”„ë¡¬í”„íŠ¸ ë©”ì‹œì§€ ìƒì„±
    - ë°˜ë“œì‹œ PlantUML ì½”ë“œ ë¸”ë¡ë§Œ ë°˜í™˜í•˜ë„ë¡ ê°•ì œ
    """
    return [
        {
            "role": "system",
            "content": (
                "ë‹¹ì‹ ì€ ì‹œí€€ìŠ¤ ë‹¤ì´ì–´ê·¸ë¨(PlantUML) ìƒì„±ê¸°ì…ë‹ˆë‹¤.\n"
                "ì¶œë ¥ì€ ë°˜ë“œì‹œ PlantUML ì½”ë“œ(@startuml ... @enduml)ë§Œ ë°˜í™˜í•´ì•¼ í•˜ë©°, "
                "ì„¤ëª…ì´ë‚˜ ì£¼ì„, JSON, ``` ì½”ë“œë¸”ë¡ ë“± ë‹¤ë¥¸ í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•˜ì§€ ë§ˆì‹­ì‹œì˜¤."
            )
        },
        {
            "role": "user",
            "content": f"""
ì•„ë˜ refined_stepsë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹œí€€ìŠ¤ ë‹¤ì´ì–´ê·¸ë¨ì„ PlantUML ì½”ë“œë¡œ ë³€í™˜í•˜ì‹œì˜¤.

ê·œì¹™:
- ê° ê³ ìœ  actor/participantëŠ” actor ë˜ëŠ” participantë¡œ ì„ ì–¸
- ê° í™”ì‚´í‘œ ë¼ë²¨ì— latency_ms í¬í•¨ (ì˜ˆ: "POST /subscribe\\n120ms")
- statusê°€ 4xx/5xxì¸ stepì€ í•´ë‹¹ í™”ì‚´í‘œ ë’¤ì— #red ìƒ‰ìƒ ì£¼ì„ ì¶”ê°€
- confidence ê°™ì€ ë©”íƒ€ ì •ë³´ëŠ” PlantUML ì£¼ì„("' ...")ìœ¼ë¡œë§Œ í‘œì‹œ
- IN-REQ, OUT-REQëŠ” í™”ì‚´í‘œ -> , IN-RES, OUT-RESëŠ” í™”ì‚´í‘œ --> ì‚¬ìš©.
- ì¶œë ¥ì€ ë°˜ë“œì‹œ @startuml ... @enduml ë¸”ë¡ë§Œ ë°˜í™˜

ì…ë ¥:
{json.dumps(refined_steps, ensure_ascii=False, indent=2)}

ì¶œë ¥ ì˜ˆì‹œ:
@startuml
actor "Skylife-API"
participant "PICASO-GW"
Skylife-API -> PICASO-GW : POST /subscribe\\n120ms
@enduml
"""
        }
    ]

# í•¨ìˆ˜: OpenAI í˜¸ì¶œ
def call_openai(messages):
 
    response = openai_client.chat.completions.create(
        model=AZURE_DEPLOYMENT_MODEL,
        messages=messages,
        temperature=1.0,
        max_tokens=1500
    )
    # print(response.choices[0].message.content)
    return response.choices[0].message.content

def logs_to_plantuml(trace_logs: List[Dict[str, Any]]) -> str:
    """
    ë‹¨ì¼ íŠ¸ëœì­ì…˜ ë¡œê·¸(trace_logs)ë¥¼ ì…ë ¥ë°›ì•„
    1) Parse ë‹¨ê³„(JSON) â†’ 2) Generate ë‹¨ê³„(PlantUML)ê¹Œì§€ ì‹¤í–‰ í›„ PlantUML ì½”ë“œ ë°˜í™˜
    """
    # 1ë‹¨ê³„: Parse
    parse_prompt = build_parse_prompt(trace_logs)
    parsed_output = call_openai(parse_prompt)
    parsed_json = json.loads(parsed_output)

    # 2ë‹¨ê³„: Generate
    generate_prompt = build_generate_prompt(parsed_json)
    plantuml_code = call_openai(generate_prompt)

    return plantuml_code

# Streamlit UI
st.title("ğŸ“¡ ë¡œê·¸ ê¸°ë°˜ ì‹œí€€ìŠ¤ ë‹¤ì´ì–´ê·¸ë¨ ìƒì„±")

uploaded_file = st.file_uploader("ìƒ˜í”Œ ë¡œê·¸ íŒŒì¼ ì—…ë¡œë“œ (JSON)", type=["json","log","txt"])
if uploaded_file:
    try:
        raw = json.load(uploaded_file)
        # ì‹¤ì œ ë¡œê·¸ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
        hits = raw.get("hits", {}).get("hits", [])
        logs = [h["_source"] for h in hits if "_source" in h]

        # st.subheader("ğŸ“‚ ì „ì²˜ë¦¬ëœ ë¡œê·¸ ë¯¸ë¦¬ë³´ê¸°")
        # st.json(logs[:3])  # ì• 3ê°œë§Œ í™•ì¸

        # íŠ¸ëœì­ì…˜ ID ëª©ë¡ ì¶”ì¶œ
        trace_ids = list(set([log.get("transactionId") for log in logs if isinstance(log, dict)]))
        selected_trace = st.selectbox("ë¶„ì„í•  íŠ¸ëœì­ì…˜ ì„ íƒ", trace_ids)

        # ì„ íƒëœ íŠ¸ëœì­ì…˜ ë¡œê·¸ë§Œ í•„í„°ë§
        trace_logs = [log for log in logs if log.get("transactionId") == selected_trace]
        st.subheader("ğŸ” ì„ íƒëœ ë¡œê·¸")
        st.json(trace_logs)

    except Exception as e:
        st.error(f"íŒŒì¼ íŒŒì‹± ì˜¤ë¥˜: {e}")
        st.stop()

    if st.button("ğŸš€ End-to-End ì‹¤í–‰"):
        with st.spinner("LLM ë¶„ì„ ë° ë‹¤ì´ì–´ê·¸ë¨ ìƒì„± ì¤‘..."):
            try:
                plantuml_code = logs_to_plantuml(trace_logs)

                st.subheader("ğŸ“ˆ ì‹œí€€ìŠ¤ ë‹¤ì´ì–´ê·¸ë¨ ì½”ë“œ (PlantUML)")
                st.code(plantuml_code, language="plantuml")

                # PlantUML ì„œë²„ ë Œë”ë§ ë§í¬
                encoded = base64.b64encode(plantuml_code.encode()).decode()
                uml_url = f"http://www.plantuml.com/plantuml/svg/~1{encoded}"
                st.markdown(f"[ğŸ–¼ï¸ ë‹¤ì´ì–´ê·¸ë¨ ë³´ê¸°]({uml_url})")

            except Exception as e:
                st.error(f"End-to-End ì‹¤í–‰ ì˜¤ë¥˜: {e}")

    # if st.button("ğŸš€ ì‹œí€€ìŠ¤ ë‹¤ì´ì–´ê·¸ë¨ ìƒì„±"):
    #     with st.spinner("LLM ë¶„ì„ ì¤‘..."):
    #         parse_prompt = build_parse_prompt(trace_logs)
    #         # st.write(parse_prompt) # í”„ë¡¬í”„íŠ¸ ì¶œë ¥ í™•ì¸ìš©
    #         parsed_output = call_openai(parse_prompt)
    #         print("Parsed Output:", parsed_output)
    #         parsed_json = json.loads(parsed_output)

    #         # st.subheader("âœ… Parse ê²°ê³¼")
    #         # st.json(parsed_json)

    #         generate_prompt = build_generate_prompt(parsed_json["steps"])
    #         # st.write(generate_prompt) # í”„ë¡¬í”„íŠ¸ ì¶œë ¥ í™•ì¸ìš©
    #         plantuml_code = call_openai(generate_prompt)

    #         st.subheader("ğŸ“ˆ ì‹œí€€ìŠ¤ ë‹¤ì´ì–´ê·¸ë¨ ì½”ë“œ (PlantUML)")
    #         st.code(plantuml_code, language="plantuml")

    #         # PlantUML ì„œë²„ ë Œë”ë§ ë§í¬
    #         encoded = base64.b64encode(plantuml_code.encode()).decode()
    #         uml_url = f"http://www.plantuml.com/plantuml/svg/~1{encoded}"
    #         st.markdown(f"[ğŸ–¼ï¸ ë‹¤ì´ì–´ê·¸ë¨ ë³´ê¸°]({uml_url})")
